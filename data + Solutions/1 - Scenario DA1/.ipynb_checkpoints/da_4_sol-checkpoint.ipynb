{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import hankel, svd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy.linalg as linalg\n",
    "# from SSA import ssa # For Singular Spectrum Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e31ec8bdd584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxmeas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xmv10_359_data_1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# Entire sensor data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xmv10_359_data_1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# For training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xmv10_359_data_1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# For Threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xmv10_359_data_1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# For Detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "xmeas = pd.read_csv('xmv10_359_data_1.csv', usecols=[14], header=None ) # Entire sensor data\n",
    "data = pd.read_csv('xmv10_359_data_1.csv', usecols=[14], skiprows=None, nrows=N, header=None ) # For training\n",
    "data1 = pd.read_csv('xmv10_359_data_1.csv', usecols=[14], skiprows=N-L, nrows=(N-L)+3500, header=None ) # For Threshold \n",
    "data2 = pd.read_csv('xmv10_359_data_1.csv', usecols=[14], skiprows=(N-L)+3500, header=None ) # For Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ssa(object):\n",
    "    \n",
    "    __supported_types = (pd.Series, np.ndarray, list)\n",
    "    \n",
    "    def __init__(self, tseries, L, save_mem=True):\n",
    "        \"\"\"\n",
    "        Decomposes the given time series with a singular-spectrum analysis. Assumes the values of the time series are\n",
    "        recorded at equal intervals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tseries : The original time series, in the form of a Pandas Series, NumPy array or list. \n",
    "        L : The window length. Must be an integer 2 <= L <= N/2, where N is the length of the time series.\n",
    "        save_mem : Conserve memory by not retaining the elementary matrices. Recommended for long time series with\n",
    "            thousands of values. Defaults to True.\n",
    "        \n",
    "        Note: Even if an NumPy array or list is used for the initial time series, all time series returned will be\n",
    "        in the form of a Pandas Series or DataFrame object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tedious type-checking for the initial time series\n",
    "        if not isinstance(tseries, self.__supported_types):\n",
    "            raise TypeError(\"Unsupported time series object. Try Pandas Series, NumPy array or list.\")\n",
    "        \n",
    "        # Checks to save us from ourselves\n",
    "        self.N = len(tseries)\n",
    "        if not 2 <= L <= self.N/2:\n",
    "            raise ValueError(\"The window length must be in the interval [2, N/2].\")\n",
    "        \n",
    "        self.L = L\n",
    "        self.orig_TS = pd.Series(tseries)\n",
    "        self.K = self.N - self.L + 1\n",
    "        \n",
    "        # Embed the time series in a trajectory matrix\n",
    "        self.X = np.array([self.orig_TS.values[i:L+i] for i in range(0, self.K)]).T\n",
    "        \n",
    "        # Decompose the trajectory matrix\n",
    "        self.U, self.Sigma, VT = np.linalg.svd(self.X)\n",
    "        self.d = np.linalg.matrix_rank(self.X)\n",
    "        \n",
    "        self.TS_comps = np.zeros((self.N, self.d))\n",
    "        \n",
    "        if not save_mem:\n",
    "            # Construct and save all the elementary matrices\n",
    "            self.X_elem = np.array([ self.Sigma[i]*np.outer(self.U[:,i], VT[i,:]) for i in range(self.d) ])\n",
    "\n",
    "            # Diagonally average the elementary matrices, store them as columns in array.           \n",
    "            for i in range(self.d):\n",
    "                X_rev = self.X_elem[i, ::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.V = VT.T\n",
    "        else:\n",
    "            # Reconstruct the elementary matrices without storing them\n",
    "            for i in range(self.d):\n",
    "                X_elem = self.Sigma[i]*np.outer(self.U[:,i], VT[i,:])\n",
    "                X_rev = X_elem[::-1]\n",
    "                self.TS_comps[:,i] = [X_rev.diagonal(j).mean() for j in range(-X_rev.shape[0]+1, X_rev.shape[1])]\n",
    "            \n",
    "            self.X_elem = \"Re-run with save_mem=False to retain the elementary matrices.\"\n",
    "            \n",
    "            # The V array may also be very large under these circumstances, so we won't keep it.\n",
    "            self.V = \"Re-run with save_mem=False to retain the V matrix.\"\n",
    "        \n",
    "        # Calculate the w-correlation matrix.\n",
    "        self.calc_wcorr()\n",
    "            \n",
    "    def components_to_df(self, n=0):\n",
    "        \"\"\"\n",
    "        Returns all the time series components in a single Pandas DataFrame object.\n",
    "        \"\"\"\n",
    "        if n > 0:\n",
    "            n = min(n, self.d)\n",
    "        else:\n",
    "            n = self.d\n",
    "        \n",
    "        # Create list of columns - call them F0, F1, F2, ...\n",
    "        cols = [\"F{}\".format(i) for i in range(n)]\n",
    "        return pd.DataFrame(self.TS_comps[:, :n], columns=cols, index=self.orig_TS.index)\n",
    "            \n",
    "    \n",
    "    def reconstruct(self, indices):\n",
    "        \"\"\"\n",
    "        Reconstructs the time series from its elementary components, using the given indices. Returns a Pandas Series\n",
    "        object with the reconstructed time series.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: An integer, list of integers or slice(n,m) object, representing the elementary components to sum.\n",
    "        \"\"\"\n",
    "        if isinstance(indices, int): indices = [indices]\n",
    "        \n",
    "        ts_vals = self.TS_comps[:,indices].sum(axis=1)\n",
    "        return pd.Series(ts_vals, index=self.orig_TS.index)\n",
    "    \n",
    "    def calc_wcorr(self):\n",
    "        \"\"\"\n",
    "        Calculates the w-correlation matrix for the time series.\n",
    "        \"\"\"\n",
    "             \n",
    "        # Calculate the weights\n",
    "        w = np.array(list(np.arange(self.L)+1) + [self.L]*(self.K-self.L-1) + list(np.arange(self.L)+1)[::-1])\n",
    "        \n",
    "        def w_inner(F_i, F_j):\n",
    "            return w.dot(F_i*F_j)\n",
    "        \n",
    "        # Calculated weighted norms, ||F_i||_w, then invert.\n",
    "        F_wnorms = np.array([w_inner(self.TS_comps[:,i], self.TS_comps[:,i]) for i in range(self.d)])\n",
    "        F_wnorms = F_wnorms**-0.5\n",
    "        \n",
    "        # Calculate Wcorr.\n",
    "        self.Wcorr = np.identity(self.d)\n",
    "        for i in range(self.d):\n",
    "            for j in range(i+1,self.d):\n",
    "                self.Wcorr[i,j] = abs(w_inner(self.TS_comps[:,i], self.TS_comps[:,j]) * F_wnorms[i] * F_wnorms[j])\n",
    "                self.Wcorr[j,i] = self.Wcorr[i,j]\n",
    "    \n",
    "    def plot_wcorr(self, min=None, max=None):\n",
    "        \"\"\"\n",
    "        Plots the w-correlation matrix for the decomposed time series.\n",
    "        \"\"\"\n",
    "        if min is None:\n",
    "            min = 0\n",
    "        if max is None:\n",
    "            max = self.d\n",
    "        \n",
    "        if self.Wcorr is None:\n",
    "            self.calc_wcorr()\n",
    "        \n",
    "        ax = plt.imshow(self.Wcorr)\n",
    "        plt.xlabel(r\"$\\tilde{F}_i$\")\n",
    "        plt.ylabel(r\"$\\tilde{F}_j$\")\n",
    "        plt.colorbar(ax.colorbar, fraction=0.045)\n",
    "        ax.colorbar.set_label(\"$W_{i,j}$\")\n",
    "        plt.clim(0,1)\n",
    "        \n",
    "        # For plotting purposes:\n",
    "        if max == self.d:\n",
    "            max_rnge = self.d-1\n",
    "        else:\n",
    "            max_rnge = max\n",
    "        \n",
    "        plt.xlim(min-0.5, max_rnge+0.5)\n",
    "        plt.ylim(max_rnge+0.5, min-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used as of now\n",
    "# def standardize_values(xmeas, data, data1, data2):\n",
    "#     xmeas = StandardScaler().fit_transform(xmeas)\n",
    "#     data = StandardScaler().fit_transform(data)\n",
    "#     data1 = StandardScaler().fit_transform(data1)\n",
    "#     data2 = StandardScaler().fit_transform(data2)\n",
    "#     return xmeas, data, data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screeplot(matrix):\n",
    "    eigenValues, eigenVectors = linalg.eig(np.matmul(matrix, matrix.T))\n",
    "    idx = eigenValues.argsort()[::-1]   \n",
    "    eigenValues = eigenValues[idx]\n",
    "    eigenVectors = eigenVectors[:,idx]\n",
    "    plt.title('Scree Plot')\n",
    "    plt.plot(eigenValues) \n",
    "    print(eigenVectors.shape)\n",
    "    U = eigenVectors[:,:r]\n",
    "    print(U.shape)\n",
    "    UT = U.T\n",
    "    print(UT.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_signal(data):\n",
    "    data = np.asarray(data).ravel()\n",
    "    data_ssa = ssa(data, L) # Singular Spectrum Analysis of data with Lag window 'L'\n",
    "    data_components = data_ssa.components_to_df()\n",
    "    data_extracted = data_ssa.reconstruct(0) # Reconstructing the data with first principal component\n",
    "    return data_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500 # Total training data points\n",
    "L = 250 # Lag Vector normallly be N/2\n",
    "k = N-L+1 # k Lag vectors for constructing trajectory matrix\n",
    "r = 1 # As per para Section 2.8 of paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_signal(data)\n",
    "X_train = hankel(data[:L],data[L-1:]) # X.shape = (250,251)\n",
    "# X_train_ssa = ssa(np.asarray(data).ravel(), L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Matrix and its Singular Value Decomposition\n",
    "X_train = hankel(data[:L],data[L-1:]) # X.shape = (250,251)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the reconstructed matrix from SVD\n",
    "# from numpy import zeros,diag,dot\n",
    "# Sigma = zeros((principalDf.shape[0], principalDf.shape[1]))\n",
    "# Sigma[:principalDf.shape[1], :principalDf.shape[0]] = diag(s)\n",
    "# U.dot(Sigma.dot(VT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Centroid\n",
    "centroid = np.mean(X_train, axis = 1)\n",
    "centroid = np.matmul(UT, centroid) # Projecting centroid to signal space\n",
    "centroid = centroid[:,np.newaxis] # centroid.shape  (250, 1)\n",
    "print('Centroid Shape', centroid.shape)\n",
    "\n",
    "# Computing Alarm Threshold (Theta) for data points from 501 to 4000\n",
    "data1 = extract_signal(data1)\n",
    "Xj = hankel(data1[:L],data1[L-1:]) # Xj.shape  (250, 3251)\n",
    "# Xj = StandardScaler().fit_transform(Xj)\n",
    "pXj = np.matmul(UT,Xj)\n",
    "dep_matrix = centroid - pXj # dep_matrix.shape (250, 3251)\n",
    "# dep_matrix_projected = np.matmul(UT, dep_matrix)\n",
    "dep_scores = np.linalg.norm(dep_matrix, axis=0, ord=2) # dep_scores.shape (3251, 1) \n",
    "D_threshold = np.max(dep_scores)\n",
    "print('Min D Score ', np.min(dep_scores))\n",
    "print('Departure Threshold is ', D_threshold)\n",
    "print('Centroid ->',centroid.shape, 'Xj ->', Xj.shape, 'Dep_Scores->', dep_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test points from 4001 to 4800\n",
    "data2 = extract_signal(data2)\n",
    "X_test = hankel(data2[:L],data2[L-1:]) # X_test.shape (250, 552)\n",
    "# X_test = StandardScaler().fit_transform(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_test = UT.dot(X_test)\n",
    "dep_matrix_test = centroid - pX_test\n",
    "# dep_matrix_test_projected = np.matmul(UT, dep_matrix_test)\n",
    "dep_scores_test = np.linalg.norm(dep_matrix_test, axis=0, ord=2)\n",
    "print('Max of dep_scores_test', np.max(dep_scores_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_scores_test = dep_scores_test[:,np.newaxis]\n",
    "dep_scores = dep_scores[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dep_scores_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Plots and Subplots\n",
    "style.use('default')\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax2 = fig.add_subplot(212)\n",
    "\n",
    "ax1.set_xlim(0,5000)\n",
    "ax2.set_xlim(0,5000)\n",
    "# ax2.set_ylim(0,.2)\n",
    "\n",
    "plt.subplots_adjust(hspace =0.3)\n",
    "\n",
    "xlables = list(range(0,5000,10)) # for both plots\n",
    "\n",
    "# Plotting signal xmeas(5)\n",
    "xmeasx_1 = list(range(501))\n",
    "xmeasx_2 = list(range(501, 4001))\n",
    "xmeasx_3 = list(range(4001,len(xmeas)))\n",
    "ax1.plot(xmeasx_1, xmeas[:501] ,'b', label='Training Phase') # Plot of Training Data\n",
    "ax1.plot(xmeasx_2, xmeas[501:4001] ,'k', label='Threshold calculation Phase') # Plot of Threshold Determination Data\n",
    "ax1.plot(xmeasx_3, xmeas[4001:] ,'r', label='Detection Phase') # Plot of Detection Phase\n",
    "ax1.plot(data, 'g', linewidth=3, label='Extracted Signal')\n",
    "ax1.set_xticklabels(xlables)\n",
    "ax1.title.set_text('Direct Attack 1 Scenario')\n",
    "ax1.set(ylabel='XMEAS(15) Reading')\n",
    "# box = ax1.get_position()\n",
    "# ax1.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "#                  box.width, box.height * 0.9])\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "\n",
    "# Plotting departure score\n",
    "dy = dep_scores\n",
    "dx = list(range(500,len(dy)+500))\n",
    "ax2.plot(dx, dy, 'b', label='Threshold calculation Phase')\n",
    "dy = dep_scores_test\n",
    "dx = list(range(4000,len(dy)+4000))\n",
    "ax2.set_xticklabels(xlables)\n",
    "ax2.hlines(D_threshold,0,5000,linestyles='dashed', label='Alarm Threshold')\n",
    "ax2.set(xlabel='Time in hours', ylabel='Departure Score')\n",
    "ax2.plot(dx, dy, 'r', label='Detection Phase')\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
